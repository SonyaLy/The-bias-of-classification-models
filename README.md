# Analysis  of approaches  to assessing the bias of classification models

The purpose of this thesis is to study and analyze approaches to assessing the bias of binary classification models from the field of information security, to determine the approach to measuring fairness. The subject of the study are metrics that allow measuring the validity of models. Tasks to be solved in the course of the study:
1.	Definition of the concept of bias and fairness of models.
2.	Classification of biases that occur in machine learning models and identification of their causes.
3.	Analysis of existing metrics for assessing the quality and bias of models.
4.	Construction of training data from the field of information security and training of binary classification models.
5.	Determination of the problem of auditing models and the presence of injustice in the data set.
6.	Development of a tool that uses the considered metrics, allowing you to analyze the trained models and draw the appropriate conclusions.

In the course of the work, classification models trained on a data set that hides bias were investigated. Modern research in the field of assessing the bias of machine learning models was analyzed.

As a result of the work, a tool was developed that allows assessing changes in equity indicators in the process of developing a classification model and comparing models in terms of equity indicators with each other. The work of this tool was demonstrated on a model for classifying malicious and safe web pages, and conclusions were drawn about the presence of a protected attribute and about the algorithm that is most effective in the task of eliminating model bias.

The results obtained can be used as a basis for integrating fairness assessment into the modeling process as a form of continuous improvement in the learning process.  


dataset: SINGH, AMIT KUMAR (2020), “Dataset of Malicious and Benign Webpages”, Mendeley Data, V2, doi: 10.17632/gdx3pkwp47.2
